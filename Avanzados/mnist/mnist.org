#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper]
#+TITLE: MNIST
#+AUTHOR: Alejandro Alcalde
#+LANGUAGE: es
#+LATEX_COMPILER: xelatex
#+OPTIONS: H:6

* Preprocesamiento

Tras leer los datos, se redimensionan para que la red neuronal pueda aceptar los datos. Para ello se crea una matriz de cuatro dimensiones con formato [ejemplos][pixels][ancho][alto]. Tras esto se normalizan los valores para que pasen del rango 0-255 a 0-1. Por último, se convierten a categóricas las clases.

#+BEGIN_SRC python
X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')

X_train = X_train / 255
X_test = X_test / 255

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
#+END_SRC


* Modelo

Para este ejercicio se ha creado el siguiente modelo:

#+BEGIN_SRC python
  def baseline_model():
     model = Sequential()
     model.add(Conv2D(32, kernel_size=(3, 3),
                      activation='relu',
                      input_shape=(1, 28, 28)))
     model.add(Conv2D(64, (3, 3), activation='relu'))
     model.add(MaxPooling2D(pool_size=(2, 2)))
     model.add(Dropout(0.25))
     model.add(Flatten())
     model.add(Dense(128, activation='relu'))
     model.add(Dropout(0.5))
     model.add(Dense(num_classes, activation='softmax'))

     # Compile model
     model.compile(loss='categorical_crossentropy',
                   optimizer='adam',
                   metrics=['accuracy'])
     return model
#+END_SRC

La primera capa oculta es una capa convolucional 2D, de 32 correspondencias entre características, con un tamaño de 3x3 y función de activación ReLu. Al ser la capa de entrada hay que definirle la estructura de los datos, [pixels][ancho][alto].
La siguiente capa es otra convolucional 2D, pero de 64 características y un tamaño de 3x3. Después hay una capa de Max pooling de tamaño 2x2, esta capa se queda con el valor más alto de la matriz 2x2. El resto de capas ocultas son una combinación de capas de regularización usando dropout/flattern y capas completamente conectadas. La última capa tiene 10 neuronas, el número de clases a predecir.

Para el modelo usando data augmentation se ha mantenido la misma estructura de la red, y se han generado imágenes rotadas 30 grados:

#+BEGIN_SRC python
datagen = ImageDataGenerator(rotation_range = 30)
fit parameters from data
datagen.fit(X_train)
#+END_SRC



* Resultados

|         | Sin Data Aug | Con Data Aug |
|---------+--------------+--------------|
| % error |        1.08% | 2%           |



* Bibliografía

- https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/
- https://machinelearningmastery.com/image-augmentation-deep-learning-keras/
